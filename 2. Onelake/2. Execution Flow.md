# OneLake Behavior and Execution Flow

*   Understanding the internal behavior of OneLake is key to leveraging its full potential. This section describes the step-by-step processes for how data is written, read, and secured, and how shortcuts function as transparent pointers to remote data. The execution flow varies depending on the compute engine and the operation being performed.

> [!NOTE]
> The core design principle of OneLake is "one copy, many engines." The behavior and execution flows are optimized to allow different compute engines (Spark, T-SQL, Power BI) to work on the same physical data files without needing to move or duplicate them.

## ## Write Path and Data Ingestion Flow

*   The process of writing data to a OneLake table involves several steps that ensure transactional integrity, data quality, and optimized storage for future reads. This flow is managed by the Delta Lake protocol.

*   **Step 1: Write Operation Initiated**
    *   A user or process initiates a write operation using a compute engine like Apache Spark in a Fabric Notebook or through a Dataflow Gen2.
    *   This could be an `INSERT`, `UPDATE`, `DELETE`, `MERGE`, or `CREATE TABLE AS SELECT` (CTAS) operation.

*   **Step 2: Staging New Data Files**
    *   The compute engine processes the new data and writes it out as one or more new Parquet files in the table's directory within OneLake.
    *   These new files are written with V-Order optimization applied by default (unless disabled), which pre-sorts and organizes the data for efficient reads.

*   **Step 3: Transactional Commit to the Delta Log**
    *   Once the new Parquet files are successfully written, the engine creates a new JSON file in the `_delta_log` subfolder of the table directory.
    *   This JSON file represents a single atomic commit and contains metadata about the transaction, including:
        *   **`add` actions:** A list of the new Parquet files that were added.
        *   **`remove` actions:** A list of existing Parquet files that are now obsolete as a result of an `UPDATE` or `DELETE` operation (these are logical deletions; the files are not yet physically deleted).
        *   Transaction details like the timestamp, user ID, and operation type.

*   **Step 4: Atomic Commit and Versioning**
    *   The write to the `_delta_log` is an atomic operation. The successful creation of the new JSON file (e.g., `00000000000000000001.json`) officially commits the transaction.
    *   At this point, any new query against the table will read this latest log file, discover the newly added Parquet files, and ignore the obsolete ones. This ensures that readers always see a consistent snapshot of the data.
    *   The table is now at a new version, enabling features like "time travel" to query previous states.

> [!IMPORTANT]
> The Delta Log is the single source of truth for a table in OneLake. It is what provides ACID (Atomicity, Consistency, Isolation, Durability) guarantees, ensuring that data remains reliable even with concurrent read and write operations.

## ## Read Path and Query Execution Flow

*   The read path describes how different compute engines access and process data from OneLake. The process is optimized to be as efficient as possible, minimizing the amount of data that needs to be scanned from storage.

### ### T-SQL and Spark Read Flow

*   **Step 1: Query Submission**
    *   A user submits a query, for example, a `SELECT` statement via the SQL Analytics Endpoint or a Spark SQL query in a notebook.

*   **Step 2: Metadata Discovery via Delta Log**
    *   The query engine first consults the `_delta_log` for the target table.
    *   It determines the latest version of the table and compiles a list of all the active Parquet files that constitute the current state of the table.

*   **Step 3: Predicate Pushdown and File Pruning**
    *   The engine analyzes the query's `WHERE` clause (the predicates).
    *   It uses the metadata stored in the Delta Log (including column-level statistics like min/max values for each file) to perform **data skipping**. If a file's statistics indicate it cannot contain any data matching the `WHERE` clause, the engine skips reading that file entirely.
    *   If the table is partitioned, the engine performs **partition pruning**, ignoring all partition folders that do not match the query filters.

*   **Step 4: Data Retrieval and Processing**
    *   The engine reads only the necessary Parquet files from OneLake storage.
    *   For the T-SQL engine and Power BI, the V-Order optimization within the files allows the Verti-Scan engine to read the data extremely efficiently, often avoiding the need to decompress entire row groups.
    *   The data is then processed in memory by the engine to produce the final result set.

### ### Power BI DirectLake Read Flow

*   DirectLake is a high-performance mode that bypasses the traditional SQL query engine for unparalleled speed.

*   **Step 1: Semantic Model Load**
    *   When a Power BI semantic model is in DirectLake mode, it does not import the data. Instead, it directly loads the metadata about the Delta table files from OneLake into memory.
    *   It analyzes the V-Ordered Parquet files and creates a map of where all the data resides in OneLake.

*   **Step 2: DAX Query Execution**
    *   A user interacts with a Power BI report, which generates a DAX query.
    *   The Power BI Analysis Services engine, running in Fabric, receives the DAX query.

*   **Step 3: Direct Data Paging from OneLake**
    *   Instead of translating DAX to SQL, the engine uses its in-memory metadata to determine exactly which segments of which V-Ordered Parquet files it needs to answer the query.
    *   It then issues direct, parallel read requests to OneLake to fetch only the required columns and rows. This process is called "paging" data on-demand from the lake into memory.

*   **Step 4: In-Memory Aggregation**
    *   The paged data is loaded into the engine's memory and processed to compute the result for the DAX query.

> [!NOTE]
> The key to DirectLake's performance is that it combines the speed of in-memory (Import) mode with the real-time nature of DirectQuery mode. It achieves this by treating OneLake as if it were an extension of its own memory, thanks to the V-Order optimization.

## ## Shortcut Execution Flow

*   When a query engine encounters a shortcut, it transparently follows the link to the target data.

*   **Step 1: Path Resolution**
    *   An engine (e.g., Spark) attempts to access a path like `/MyWorkspace/MyLakehouse.Lakehouse/Files/MyShortcut/data.csv`.
    *   OneLake detects that `MyShortcut` is a shortcut object, not a physical folder.

*   **Step 2: Metadata Lookup**
    *   OneLake retrieves the shortcut's metadata, which contains the target path (e.g., an S3 or ADLS Gen2 URI) and the associated cloud connection reference.

*   **Step 3: Credential Delegation and Authentication**
    *   OneLake uses the stored credentials from the cloud connection to authenticate to the external data source (e.g., ADLS Gen2 or S3) on behalf of the user or the service.
    *   This is a secure delegation process; the query engine itself does not need to handle the credentials.

*   **Step 4: Data Access**
    *   Once authenticated, OneLake establishes a connection and the query engine can read or write data to the target path as if it were a local directory in OneLake. The entire process is transparent to the user and the code.

## ## Security Enforcement Flow

*   OneLake enforces security at multiple layers. Every request to access data goes through a series of checks.

*   **Step 1: Workspace Role Check**
    *   The first check is at the workspace level. The user must be at least a **Viewer** in the workspace containing the data item to proceed.
    *   If the user is an **Admin**, **Member**, or **Contributor**, they are typically granted full read/write access to the underlying data files, and further data-level checks may be bypassed for file system operations.

*   **Step 2: Item Sharing and Permissions Check**
    *   If the user is accessing an item that was shared with them directly (e.g., given SQL read permissions), this grant is evaluated.

*   **Step 3: OneLake Data Access Role Check (for Viewers)**
    *   If the user has the Viewer role, OneLake then evaluates the more granular security.
    *   It checks if the user is a member of any OneLake Data Access Roles defined on the Lakehouse.
    *   If a role grants access to the specific folder or table being queried, the check passes.

*   **Step 4: RLS/CLS and T-SQL Permissions Check**
    *   If the access is through the SQL endpoint, any table-level Row-Level Security (RLS) or Column-Level Security (CLS) predicates are applied to the query.
    *   Standard T-SQL permissions (`GRANT`, `REVOKE`) are also evaluated.

*   **Step 5: Access Granted or Denied**
    *   If the user passes all the necessary checks for their specific operation and access method, the data is returned.
    *   If any check fails, the operation is blocked, and an "access denied" error is returned.

## ## Flashcards (Q&A)

*   **Q: What is the single source of truth for a Delta table in OneLake?**
    *   A: The `_delta_log` folder, which contains a transactional history of all changes to the table.
*   **Q: How does OneLake provide ACID transactions?**
    *   A: Through the Delta Lake protocol, where every change is recorded as an atomic commit in the transaction log (`_delta_log`).
*   **Q: What is the first step a query engine takes when reading a Delta table?**
    *   A: It reads the `_delta_log` to determine the list of active Parquet files that make up the current version of the table.
*   **Q: What is the main difference between Power BI's DirectLake and DirectQuery modes?**
    *   A: DirectQuery translates DAX to SQL queries, which are run by a SQL engine. DirectLake bypasses the SQL engine and pages V-Ordered data directly from OneLake into memory for processing.
*   **Q: How does a shortcut handle authentication to an external source like S3?**
    *   A: It uses a pre-configured cloud connection that securely stores the necessary credentials. OneLake delegates the authentication using this connection on behalf of the user.
*   **Q: Is the process of following a shortcut visible to the user's code?**
    *   A: No, the process is transparent. The shortcut appears as a local folder, and the code can interact with it without knowing it points to an external location.
*   **Q: Which workspace role is most affected by OneLake Data Access Roles?**
    *   A: The **Viewer** role, as these granular roles are designed to restrict or grant specific data access to users who otherwise only have read-only permissions on the workspace.
*   **Q: What two optimization techniques does a query engine use to reduce the amount of data it reads?**
    *   A: **Partition pruning** (skipping entire folders based on filters) and **data skipping** (using file-level statistics to avoid reading irrelevant files).
*   **Q: What does it mean for a file to be "logically deleted" in a Delta table?**
    *   A: It means the file has been marked as obsolete in the `_delta_log` by a `remove` action. The physical file still exists on storage (until `VACUUM` is run) but is no longer part of the active table version.
*   **Q: What enables DirectLake to achieve in-memory-like performance?**
    *   A: The combination of V-Order optimization on the Parquet files and the Power BI engine's ability to intelligently page only the necessary data from OneLake into memory on demand.
