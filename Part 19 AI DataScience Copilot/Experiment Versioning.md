# [Experiment Versioning](Part 19 AI DataScience Copilot/Experiment Versioning.md)

Canonical documentation for [Experiment Versioning](Part 19 AI DataScience Copilot/Experiment Versioning.md). This document defines concepts, terminology, and standard usage.

## Purpose
[Experiment Versioning](Part 19 AI DataScience Copilot/Experiment Versioning.md) exists to ensure the reproducibility, auditability, and comparability of iterative tests, particularly within the domains of data science, machine learning, and software optimization. In complex systems, an "experiment" is rarely a static event; it is a series of iterations where variables, code, data, and environments evolve. 

Without formal versioning, organizations face "experimental drift," where results cannot be reliably traced back to their specific configurations. This documentation establishes a framework for capturing the state of an experiment at a specific point in time, allowing stakeholders to validate findings, revert to previous states, and maintain a clear lineage of intellectual property.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative.

## Scope
Clarify what is in scope and out of scope for this topic.

**In scope:**
* **State Capture:** The methodology for recording code, data, and configuration states.
* **Lineage and Provenance:** Tracking the evolution of experiments over time.
* **Reproducibility Standards:** Requirements for ensuring an experiment can be re-executed with identical results.
* **Metadata Management:** The structured data required to describe an experiment version.

**Out of scope:**
* **Specific Vendor Implementations:** Detailed guides for tools like MLflow, DVC, Weights & Biases, or Neptune.ai.
* **Statistical Methodology:** The mathematical theory behind A/B testing or hypothesis testing.
* **Deployment Orchestration:** The mechanics of how a versioned model or feature is pushed to production.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| **Experiment** | A systematic procedure undertaken to test a hypothesis or evaluate the performance of a specific configuration. |
| **Version** | A unique, immutable snapshot of all components (code, data, config) required to execute an experiment. |
| **Run (or Trial)** | A single execution of an experiment version. One version may have multiple runs (e.g., for stochastic validation). |
| **Artifact** | Any output generated by an experiment run, such as logs, trained models, plots, or processed datasets. |
| **Lineage** | The historical record of how one experiment version evolved from or relates to previous versions. |
| **Metadata** | Structured information describing the experiment (e.g., author, timestamp, hardware specs, hyperparameters). |
| **Immutability** | The principle that once a version is defined and executed, its parameters and code cannot be altered. |

## Core Concepts

### 1. The Reproducibility Triad
Experiment versioning relies on the synchronization of three distinct pillars:
*   **Code:** The logic, scripts, and dependencies used to execute the experiment.
*   **Data:** The specific dataset version, including features, labels, and any preprocessing transformations.
*   **Configuration:** The externalized parameters (hyperparameters, environment variables, hardware constraints) that influence execution.

### 2. Immutability of Versions
A version must be immutable. If any component of the Reproducibility Triad changes, a new version must be generated. This prevents "silent failures" where results change despite the version identifier remaining the same.

### 3. Traceability and Lineage
Versioning must provide a "paper trail." This includes knowing which version of a dataset was used by which version of the code to produce a specific artifact. Lineage allows for "root cause analysis" when an experiment produces unexpected or anomalous results.

## Standard Model

The standard model for [Experiment Versioning](Part 19 AI DataScience Copilot/Experiment Versioning.md) follows a hierarchical structure:

1.  **Project/Namespace:** The highest level of organization (e.g., "Recommendation Engine Optimization").
2.  **Experiment:** A specific hypothesis or approach (e.g., "Collaborative Filtering vs. Content-Based").
3.  **Version:** A specific configuration of the experiment (e.g., "v1.2.0 - Learning Rate 0.01").
4.  **Run:** The execution instance. Each run inherits the version's properties and records specific outcomes (metrics).

### The Versioning Schema
While semantic versioning (Major.Minor.Patch) is common, experiment versioning often utilizes **Hash-based Versioning**. In this model, a unique cryptographic hash is generated from the combination of code commit ID, data URI, and configuration file. This ensures that even the slightest change results in a new, unique identifier.

## Common Patterns

### Git-based Versioning
Using Git commits as the anchor for code state. The experiment version is often tagged or linked to a specific commit SHA.

### Manifest-based Versioning
A central "Manifest" file (JSON or YAML) acts as the source of truth. It explicitly lists the versions of data, code, and environment (e.g., Docker image hash) used.

### Parameter Sweeping (Grid/Random Search)
In this pattern, a single "Parent Version" spawns multiple "Child Runs," each with varying parameters. The versioning system must track the relationship between the parent hypothesis and the individual parameter iterations.

## Anti-Patterns

*   **The "Latest" Tag:** Using a mutable pointer like `latest` to refer to data or code. This destroys reproducibility as "latest" changes over time.
*   **Manual Logging:** Recording experiment parameters in spreadsheets or documents outside of the versioning system, leading to "lost" configurations.
*   **Hidden State:** Relying on local environment variables or uncommitted code changes that are not captured in the version snapshot.
*   **Post-hoc Versioning:** Attempting to assign a version to an experiment after it has been run, rather than defining the version as a prerequisite for execution.

## Edge Cases

*   **Non-Deterministic Algorithms:** Algorithms that produce different results even with identical inputs (e.g., certain neural network initializations). Versioning must capture the "Seed" value to mitigate this.
*   **Streaming Data:** When experiments run against live data streams, "Data Versioning" becomes a temporal window (start/end timestamp) rather than a static snapshot.
*   **Hardware Heterogeneity:** Differences in GPU architecture or CPU instructions can lead to floating-point variations. In high-precision environments, hardware specifications must be part of the version metadata.
*   **External API Dependencies:** If an experiment calls an external service (e.g., a translation API), the version of that external service is often outside the experimenter's control, creating a "versioning leak."

## Related Topics
*   **Data Versioning:** The specific discipline of managing changes to large-scale datasets.
*   **Model Registry:** A specialized versioning system for trained machine learning models.
*   **Configuration Management:** The broader practice of managing system states.
*   **Lineage Tracking:** The visualization and mapping of data and process flows.

## Change Log

| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-18 | Initial AI-generated canonical documentation |